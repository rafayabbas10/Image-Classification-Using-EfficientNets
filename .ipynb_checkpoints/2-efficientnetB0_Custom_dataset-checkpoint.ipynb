{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'real']\n",
      "Types of classes labels found:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset')\n",
    "\n",
    "print (dataset_path)  #what kinds of classes are in this dataset\n",
    "\n",
    "print(\"Types of classes labels found: \", len(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_classes = os.listdir('dataset' + '/' +item)\n",
    " #print(all_classes)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_classes:\n",
    "    class_labels.append((item, str('dataset_path' + '/' +item) + '/' + room))\n",
    "    #print(class_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Labels                         image\n",
      "0     ai  dataset_path/ai/img(100).jpg\n",
      "1     ai  dataset_path/ai/img(101).jpg\n",
      "2     ai  dataset_path/ai/img(102).jpg\n",
      "3     ai  dataset_path/ai/img(103).jpg\n",
      "4     ai  dataset_path/ai/img(104).jpg\n",
      "    Labels                           image\n",
      "962   real  dataset_path/real/img (95).jpg\n",
      "963   real  dataset_path/real/img (96).jpg\n",
      "964   real  dataset_path/real/img (97).jpg\n",
      "965   real  dataset_path/real/img (98).jpg\n",
      "966   real  dataset_path/real/img (99).jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a dataframe        \n",
    "df = pd.DataFrame(data=class_labels, columns=['Labels', 'image'])\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the dataset:  967\n",
      "Labels\n",
      "real    487\n",
      "ai      480\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's check how many samples for each category are present\n",
    "print(\"Total number of images in the dataset: \", len(df))\n",
    "\n",
    "label_count = df['Labels'].value_counts()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images successfully loaded: 967\n",
      "Total images skipped due to errors: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "path = 'dataset/'\n",
    "dataset_path = os.listdir('dataset')\n",
    "\n",
    "im_size = 224\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Keep track of problematic files\n",
    "skipped_files = []\n",
    "\n",
    "for i in dataset_path:\n",
    "    data_path = path + str(i)  \n",
    "    filenames = [i for i in os.listdir(data_path)]\n",
    "   \n",
    "    for f in filenames:\n",
    "        try:\n",
    "            img = cv2.imread(data_path + '/' + f)\n",
    "            \n",
    "            # Check if image was loaded successfully\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image file: {data_path}/{f}\")\n",
    "                skipped_files.append(f\"{data_path}/{f}\")\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, (im_size, im_size))\n",
    "            images.append(img)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {data_path}/{f}: {e}\")\n",
    "            skipped_files.append(f\"{data_path}/{f}\")\n",
    "\n",
    "print(f\"Total images successfully loaded: {len(images)}\")\n",
    "print(f\"Total images skipped due to errors: {len(skipped_files)}\")\n",
    "\n",
    "if skipped_files:\n",
    "    print(\"First 10 skipped files:\")\n",
    "    for i, file in enumerate(skipped_files[:10]):\n",
    "        print(f\"  {i+1}. {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#This model takes input images of shape (224, 224, 3), and the input data should range [0, 255]. \n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "images = images.astype('float32') / 255.0\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai' 'ai'\n",
      " 'ai' 'ai' 'ai' 'ai' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real'\n",
      " 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real' 'real']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "y=df['Labels'].values\n",
    "print(y)\n",
    "\n",
    "y_labelencoder = LabelEncoder ()\n",
    "y = y_labelencoder.fit_transform (y)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y=y.reshape(-1,1)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "Y = ct.fit_transform(y) #.toarray()\n",
    "print(Y[:5])\n",
    "print(Y[35:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (773, 224, 224, 3), (773, 2)\n",
      "Validation set: (145, 224, 224, 3), (145, 2)\n",
      "Test set: (49, 224, 224, 3), (49, 2)\n",
      "\n",
      "Class distribution in training set:\n",
      "(array([0, 1], dtype=int64), array([384, 389], dtype=int64))\n",
      "\n",
      "Class distribution in validation set:\n",
      "(array([0, 1], dtype=int64), array([72, 73], dtype=int64))\n",
      "\n",
      "Class distribution in test set:\n",
      "(array([0, 1], dtype=int64), array([24, 25], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Modified train/validation/test split with stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: training vs (validation+test) - with stratification\n",
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    images, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=415,\n",
    "    stratify=np.argmax(Y, axis=1)  # Ensure class balance in splits\n",
    ")\n",
    "\n",
    "# Second split: validation vs test - with stratification\n",
    "val_x, test_x, val_y, test_y = train_test_split(\n",
    "    val_x, val_y, \n",
    "    test_size=0.25, \n",
    "    random_state=415,\n",
    "    stratify=np.argmax(val_y, axis=1)  # Ensure class balance in splits\n",
    ")\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Training set: {train_x.shape}, {train_y.shape}\")\n",
    "print(f\"Validation set: {val_x.shape}, {val_y.shape}\")\n",
    "print(f\"Test set: {test_x.shape}, {test_y.shape}\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(np.unique(np.argmax(train_y, axis=1), return_counts=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(np.unique(np.argmax(val_y, axis=1), return_counts=True))\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(np.unique(np.argmax(test_y, axis=1), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# images, Y = shuffle(images, Y, random_state=1)\n",
    "\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.05, random_state=415)\n",
    "\n",
    "# #inpect the shape of the training and testing.\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator with augmentation for the training data\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the generator on the training data\n",
    "datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# EfficientNet Implementation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# Include data augmentation in the model\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Define model input\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Using pretrained weights initialization\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Freeze most base model layers but unfreeze the last few\n",
    "for layer in base_model.layers[:-20]:  # Unfreeze more layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add dropout and regularization\n",
    "x = data_augmentation(inputs)  # Apply augmentation directly in the model\n",
    "x = base_model(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)  # Larger intermediate layer\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data generators for augmentation during training\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Advanced augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # Geometric transformations\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    \n",
    "    # Color/intensity adjustments\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    channel_shift_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test data\n",
    "valid_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow(\n",
    "    train_x, train_y,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create validation generator (no augmentation)\n",
    "valid_generator = valid_datagen.flow(\n",
    "    val_x, val_y,\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Preview augmented images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_augmented_images(data_generator, num_images=5):\n",
    "    \"\"\"Plot some augmented images to verify augmentation.\"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Get a batch of augmented images\n",
    "    x_batch, y_batch = next(data_generator)\n",
    "    \n",
    "    # Plot images\n",
    "    for i in range(num_images):\n",
    "        # Extract img and label\n",
    "        img = x_batch[i]\n",
    "        label = \"AI\" if np.argmax(y_batch[i]) == 0 else \"Real\"\n",
    "        \n",
    "        # Convert to RGB if needed and clip to valid range\n",
    "        img_to_show = np.clip(img, 0, 1)\n",
    "        \n",
    "        plt.subplot(1, num_images, i+1)\n",
    "        plt.imshow(img_to_show)\n",
    "        plt.title(f\"Class: {label}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show augmented samples\n",
    "plot_augmented_images(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "train_x shape: (773, 224, 224, 3)\n",
      "train_y shape: (773, 2)\n",
      "\n",
      "Validation data shapes:\n",
      "val_x shape: (145, 224, 224, 3)\n",
      "val_y shape: (145, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │           \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m655,872\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,776,485</span> (18.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,776,485\u001b[0m (18.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,075,314</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,075,314\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,701,171</span> (10.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,701,171\u001b[0m (10.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafay Abbas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5338 - loss: 1.0195    \n",
      "Epoch 1: val_accuracy improved from -inf to 0.50000, saving model to best_model.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step  \n",
      "\n",
      "Epoch 0 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.5329 - loss: 1.0183 - val_accuracy: 0.5000 - val_loss: 0.7025 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 916ms/step - accuracy: 0.4375 - loss: 1.0747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.4375 - loss: 1.0747 - val_accuracy: 0.4706 - val_loss: 0.7100 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4524 - loss: 1.0979       \n",
      "Epoch 3: val_accuracy improved from 0.50000 to 0.51562, saving model to best_model.keras\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4535 - loss: 1.0956 - val_accuracy: 0.5156 - val_loss: 0.6992 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.5625 - loss: 0.7487\n",
      "Epoch 4: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 573ms/step\n",
      "\n",
      "Epoch 3 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 163ms/step - accuracy: 0.5625 - loss: 0.7487 - val_accuracy: 0.3529 - val_loss: 0.7459 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5138 - loss: 0.9000 \n",
      "Epoch 5: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5124 - loss: 0.9035 - val_accuracy: 0.5078 - val_loss: 0.6987 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 954ms/step - accuracy: 0.3438 - loss: 1.1605\n",
      "Epoch 6: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.3438 - loss: 1.1605 - val_accuracy: 0.4118 - val_loss: 0.7217 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4847 - loss: 0.9353    \n",
      "Epoch 7: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 583ms/step\n",
      "\n",
      "Epoch 6 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.4855 - loss: 0.9343 - val_accuracy: 0.5156 - val_loss: 0.6949 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.9254\n",
      "Epoch 8: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5000 - loss: 0.9254 - val_accuracy: 0.3529 - val_loss: 0.7270 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4835 - loss: 0.9235 \n",
      "Epoch 9: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.4836 - loss: 0.9223 - val_accuracy: 0.5078 - val_loss: 0.6964 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 897ms/step - accuracy: 0.5938 - loss: 0.8870\n",
      "Epoch 10: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561ms/step\n",
      "\n",
      "Epoch 9 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.5938 - loss: 0.8870 - val_accuracy: 0.4118 - val_loss: 0.7138 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5103 - loss: 0.9021    \n",
      "Epoch 11: val_accuracy did not improve from 0.51562\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5100 - loss: 0.9029 - val_accuracy: 0.4766 - val_loss: 0.7054 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 859ms/step - accuracy: 0.3750 - loss: 0.8729\n",
      "Epoch 12: val_accuracy improved from 0.51562 to 0.64706, saving model to best_model.keras\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.3750 - loss: 0.8729 - val_accuracy: 0.6471 - val_loss: 0.6629 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4670 - loss: 0.9669 \n",
      "Epoch 13: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 569ms/step\n",
      "\n",
      "Epoch 12 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.4672 - loss: 0.9661 - val_accuracy: 0.5078 - val_loss: 0.6923 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 888ms/step - accuracy: 0.5625 - loss: 0.8227\n",
      "Epoch 14: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5625 - loss: 0.8227 - val_accuracy: 0.4118 - val_loss: 0.6996 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4596 - loss: 0.8791 \n",
      "Epoch 15: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.4602 - loss: 0.8783 - val_accuracy: 0.4922 - val_loss: 0.6939 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.4688 - loss: 0.8065\n",
      "Epoch 16: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 631ms/step\n",
      "\n",
      "Epoch 15 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 178ms/step - accuracy: 0.4688 - loss: 0.8065 - val_accuracy: 0.5294 - val_loss: 0.6908 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4955 - loss: 0.8312 \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.4953 - loss: 0.8317 - val_accuracy: 0.4844 - val_loss: 0.6982 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 943ms/step - accuracy: 0.5938 - loss: 0.7796\n",
      "Epoch 18: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5938 - loss: 0.7796 - val_accuracy: 0.5882 - val_loss: 0.6801 - learning_rate: 2.0000e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4843 - loss: 0.8954 \n",
      "Epoch 19: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 579ms/step\n",
      "\n",
      "Epoch 18 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - accuracy: 0.4837 - loss: 0.8947 - val_accuracy: 0.4922 - val_loss: 0.6971 - learning_rate: 2.0000e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 872ms/step - accuracy: 0.5312 - loss: 0.9876\n",
      "Epoch 20: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5312 - loss: 0.9876 - val_accuracy: 0.5294 - val_loss: 0.6902 - learning_rate: 2.0000e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5640 - loss: 0.7963    \n",
      "Epoch 21: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.5625 - loss: 0.7974 - val_accuracy: 0.5078 - val_loss: 0.6954 - learning_rate: 2.0000e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 935ms/step - accuracy: 0.6250 - loss: 0.8055\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 566ms/step\n",
      "\n",
      "Epoch 21 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.6250 - loss: 0.8055 - val_accuracy: 0.4118 - val_loss: 0.7183 - learning_rate: 2.0000e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4823 - loss: 0.8191 \n",
      "Epoch 23: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.4830 - loss: 0.8194 - val_accuracy: 0.5000 - val_loss: 0.6992 - learning_rate: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.4688 - loss: 0.8274\n",
      "Epoch 24: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.4688 - loss: 0.8274 - val_accuracy: 0.4706 - val_loss: 0.7102 - learning_rate: 1.0000e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4883 - loss: 0.8192 \n",
      "Epoch 25: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 587ms/step\n",
      "\n",
      "Epoch 24 - Validation predictions: {0: 145}\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.4894 - loss: 0.8181 - val_accuracy: 0.5078 - val_loss: 0.6991 - learning_rate: 1.0000e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 892ms/step - accuracy: 0.4062 - loss: 1.1417\n",
      "Epoch 26: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4062 - loss: 1.1417 - val_accuracy: 0.4118 - val_loss: 0.7248 - learning_rate: 1.0000e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5189 - loss: 0.8049 \n",
      "Epoch 27: val_accuracy did not improve from 0.64706\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5183 - loss: 0.8054 - val_accuracy: 0.5078 - val_loss: 0.7016 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Add these print statements here, after your data loading and preprocessing\n",
    "print(\"Training data shapes:\")\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\")\n",
    "print(\"\\nValidation data shapes:\")\n",
    "print(f\"val_x shape: {val_x.shape}\")\n",
    "print(f\"val_y shape: {val_y.shape}\")\n",
    "\n",
    "# Custom monitoring callback\n",
    "class PredictionMonitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 3 == 0:  # Check every 3 epochs\n",
    "            val_pred = np.argmax(self.model.predict(val_x), axis=1)\n",
    "            unique, counts = np.unique(val_pred, return_counts=True)\n",
    "            print(f\"\\nEpoch {epoch} - Validation predictions: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Use optimizer with learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "# Fit the generator on the training data\n",
    "train_generator = train_datagen.flow(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "valid_generator = val_datagen.flow(\n",
    "    val_x,\n",
    "    val_y,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2, \n",
    "    patience=5, \n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpoint to save best model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_x) // 32\n",
    "validation_steps = len(val_x) // 32\n",
    "\n",
    "# Train using the data generator\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=100,\n",
    "    verbose=1,  # Changed to 1 for more detailed progress\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        reduce_lr,\n",
    "        checkpoint,\n",
    "        PredictionMonitor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Visualize Training History\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\h5py\\_hl\\files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')  # Changed from .h5 to .keras\n",
    "\n",
    "# Visualize Training History\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = best_model.evaluate(test_x, test_y, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Generate predictions and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(test_x)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_y, axis=1)\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, \n",
    "                          target_names=['AI Images', 'Real Images'],\n",
    "                          zero_division=0))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['AI Images', 'Real Images'],\n",
    "                yticklabels=['AI Images', 'Real Images'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    # Fallback to matplotlib if seaborn is not available\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['AI Images', 'Real Images']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some predictions on test data\n",
    "def plot_predictions(model, x, y_true, num_images=10):\n",
    "    \"\"\"Plot some test images with their predictions.\"\"\"\n",
    "    predictions = model.predict(x)\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    # Find both correct and incorrect predictions\n",
    "    correct_idx = np.where(pred_classes == true_classes)[0]\n",
    "    incorrect_idx = np.where(pred_classes != true_classes)[0]\n",
    "    \n",
    "    # Determine how many of each to show\n",
    "    n_correct = min(num_images // 2, len(correct_idx))\n",
    "    n_incorrect = min(num_images - n_correct, len(incorrect_idx))\n",
    "    \n",
    "    # Plot correct predictions\n",
    "    for i in range(n_correct):\n",
    "        if i < len(correct_idx):\n",
    "            idx = correct_idx[i]\n",
    "            plt.subplot(2, num_images//2, i+1)\n",
    "            plt.imshow(x[idx])\n",
    "            class_name = \"AI\" if true_classes[idx] == 0 else \"Real\"\n",
    "            confidence = predictions[idx][pred_classes[idx]] * 100\n",
    "            plt.title(f\"✓ {class_name} ({confidence:.1f}%)\", color='green')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    # Plot incorrect predictions\n",
    "    for i in range(n_incorrect):\n",
    "        if i < len(incorrect_idx):\n",
    "            idx = incorrect_idx[i]\n",
    "            plt.subplot(2, num_images//2, i+n_correct+1)\n",
    "            plt.imshow(x[idx])\n",
    "            true_class = \"AI\" if true_classes[idx] == 0 else \"Real\"\n",
    "            pred_class = \"AI\" if pred_classes[idx] == 0 else \"Real\"\n",
    "            confidence = predictions[idx][pred_classes[idx]] * 100\n",
    "            plt.title(f\"✗ True: {true_class}, Pred: {pred_class} ({confidence:.1f}%)\", color='red')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample of predictions\n",
    "plot_predictions(best_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(test_x, test_y)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Efficient Model On Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "\n",
    "img_path = 'unseen.jpg'\n",
    "\n",
    "#img = image.load_img(img_path, target_size=(224, 224))\n",
    "#x = img.img_to_array(img)\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "\n",
    "x = np.expand_dims(img, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "print('Input image shape:', x.shape)\n",
    "\n",
    "my_image = imread(img_path)\n",
    "imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(x)\n",
    "preds     # probabilities for being in each of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cuda and cudnn is installed for this tensorflow version. So we can see GPU is enabled\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1 \n",
    "with tf.device('/CPU:0'):\n",
    "    cpu_performance =model.fit(train_x, train_y, epochs=30, verbose=2)\n",
    "    cpu_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%timeit -n1 -r1 \n",
    "with tf.device('/GPU:0'):\n",
    "    gpu_performance =model.fit(train_x, train_y, epochs=30, verbose=2)\n",
    "    gpu_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU completed the training in 7 min 53 Seconds and GPU did that training in 25.6 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
